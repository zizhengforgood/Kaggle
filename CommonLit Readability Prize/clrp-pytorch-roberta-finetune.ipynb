{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://uku28motab.feishu.cn/docs/doccnUDbEhudHm2V440lcY87B1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-24T06:58:41.897397Z",
     "iopub.status.busy": "2021-07-24T06:58:41.897005Z",
     "iopub.status.idle": "2021-07-24T06:58:41.904839Z",
     "shell.execute_reply": "2021-07-24T06:58:41.903710Z",
     "shell.execute_reply.started": "2021-07-24T06:58:41.897355Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "## 定义自定义magic(在notebook中使用，脚本中不生效)来保存大多数有用的类，并在推理笔记本中使用它们\n",
    "## 而不是每次在类中发生更改时都复制代码\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)\n",
    "    get_ipython().run_cell(cell)\n",
    "    \n",
    "Path('/kaggle/working/scripts').mkdir(exist_ok=True)\n",
    "models_dir = Path('/kaggle/working/models')\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "17ca5c94-c7c7-49a0-8822-05b59ceaaa5e",
    "_uuid": "d9a1a99e-de9d-4024-9715-bab69dafd9d6",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:34.781848Z",
     "iopub.status.busy": "2021-07-20T09:49:34.781313Z",
     "iopub.status.idle": "2021-07-20T09:49:42.430846Z",
     "shell.execute_reply": "2021-07-20T09:49:42.42996Z",
     "shell.execute_reply.started": "2021-07-20T09:49:34.781798Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/imports.py\n",
    "# 导入相关的库文件\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, Sampler\n",
    "from torch.nn.functional import mse_loss\n",
    "from transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-talk')\n",
    "# print(plt.style.available)\n",
    "from time import time\n",
    "from colorama import Fore, Back, Style\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "g_ = Fore.GREEN\n",
    "y_ = Fore.YELLOW\n",
    "w_ = Fore.WHITE\n",
    "bb_ = Back.BLACK\n",
    "sr_ = Style.RESET_ALL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "757a446c-b2dc-4eea-91d6-8f8bcd1155c3",
    "_uuid": "3d4bc2bb-36f6-4d6a-aa6c-b9ec39f40c45",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.435068Z",
     "iopub.status.busy": "2021-07-20T09:49:42.434815Z",
     "iopub.status.idle": "2021-07-20T09:49:42.442834Z",
     "shell.execute_reply": "2021-07-20T09:49:42.442099Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.435043Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/config.py\n",
    "# 参数配置类\n",
    "class Config:\n",
    "    model_name = 'roberta-base'\n",
    "    pretrained_model_path = '../input/clrp-pretrain/clrp_roberta_base'\n",
    "    output_hidden_states = True\n",
    "    epochs = 3\n",
    "    evaluate_interval = 10\n",
    "    batch_size = 16\n",
    "    device = 'cuda'\n",
    "    seed = 42\n",
    "    max_len = 256\n",
    "    lr = 2e-5\n",
    "    wd = 0.01\n",
    "    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.445088Z",
     "iopub.status.busy": "2021-07-20T09:49:42.444803Z",
     "iopub.status.idle": "2021-07-20T09:49:42.458172Z",
     "shell.execute_reply": "2021-07-20T09:49:42.457333Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.445056Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89d2d6a9-c024-4852-a1cc-2892d7e2aab9",
    "_uuid": "f7aa8d00-0b7b-4704-b00c-45c214001ecb",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.459849Z",
     "iopub.status.busy": "2021-07-20T09:49:42.459394Z",
     "iopub.status.idle": "2021-07-20T09:49:42.534078Z",
     "shell.execute_reply": "2021-07-20T09:49:42.533107Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.459814Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('../input/k/chamecall/train-val-split/train.csv')\n",
    "# val_df = pd.read_csv('../input/k/chamecall/train-val-split/val.csv')\n",
    "# 读取交叉验证数据\n",
    "kfold_df = pd.read_csv('../input/k/chamecall/train-val-split/kfold.csv')\n",
    "# aux_df = pd.read_csv('../input/clrauxdata/aux_data_embed.csv', index_col='index', converters={'aux_text': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "740b0f57-cad8-4758-995b-c2119dc3eac3",
    "_uuid": "f34eca01-c56c-4b8f-8c8c-5188ac148878",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.536377Z",
     "iopub.status.busy": "2021-07-20T09:49:42.536106Z",
     "iopub.status.idle": "2021-07-20T09:49:42.549501Z",
     "shell.execute_reply": "2021-07-20T09:49:42.548601Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.536352Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/dataset.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "# 将样本转换成特征\n",
    "def convert_examples_to_features(text, tokenizer, max_len):\n",
    "\n",
    "    tok = tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    return tok\n",
    "\n",
    "# 数据处理\n",
    "class CLRPDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.tolist()\n",
    "        if not is_test:\n",
    "            self.targets = self.data.target.tolist()\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt = self.excerpts[item]\n",
    "            label = self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.float),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3fff34f9-07fe-4b91-a284-a06ce99fd43f",
    "_uuid": "024b10c3-9bdc-48a6-be51-cd8f8a064738",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.553647Z",
     "iopub.status.busy": "2021-07-20T09:49:42.553407Z",
     "iopub.status.idle": "2021-07-20T09:49:42.562199Z",
     "shell.execute_reply": "2021-07-20T09:49:42.561356Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.553625Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 注意力头方法\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector\n",
    "# CLPR模型\n",
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.head = AttentionHead(self.h_size)\n",
    "        self.linear = nn.Linear(self.h_size, 1)\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "        x = self.head(transformer_out.last_hidden_state)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.564213Z",
     "iopub.status.busy": "2021-07-20T09:49:42.563831Z",
     "iopub.status.idle": "2021-07-20T09:49:42.573183Z",
     "shell.execute_reply": "2021-07-20T09:49:42.572293Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.564178Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建优化器\n",
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:197]    \n",
    "    attention_parameters = named_parameters[199:203]\n",
    "    regressor_parameters = named_parameters[203:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = Config.lr\n",
    "\n",
    "        if layer_num >= 69:        \n",
    "            lr = Config.lr * 2.5\n",
    "\n",
    "        if layer_num >= 133:\n",
    "            lr = Config.lr * 5\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return optim.AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7428183c-f79b-452c-af9f-1b76b4684032",
    "_uuid": "0a678160-5a09-49b1-8f51-fce52d18992e",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.575143Z",
     "iopub.status.busy": "2021-07-20T09:49:42.574742Z",
     "iopub.status.idle": "2021-07-20T09:49:42.604383Z",
     "shell.execute_reply": "2021-07-20T09:49:42.603407Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.575111Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 平均计数\n",
    "class AvgCounter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def update(self, loss, n_samples):\n",
    "        self.loss += loss * n_samples\n",
    "        self.n_samples += n_samples\n",
    "        \n",
    "    def avg(self):\n",
    "        return self.loss / self.n_samples\n",
    "    \n",
    "    def reset(self):\n",
    "        self.loss = 0\n",
    "        self.n_samples = 0\n",
    "# 验证调度\n",
    "class EvaluationScheduler:\n",
    "    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n",
    "        self.evaluation_schedule = evaluation_schedule\n",
    "        self.evaluation_interval = self.evaluation_schedule[0][1]\n",
    "        self.last_evaluation_step = 0\n",
    "        self.prev_loss = float('inf')\n",
    "        self.penalize_factor = penalize_factor\n",
    "        self.penalty = 0\n",
    "        self.prev_interval = -1\n",
    "        self.max_penalty = max_penalty\n",
    "\n",
    "    def step(self, step):\n",
    "        # should we to make evaluation right now\n",
    "        if step >= self.last_evaluation_step + self.evaluation_interval:\n",
    "            self.last_evaluation_step = step\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "            \n",
    "    def update_evaluation_interval(self, last_loss):\n",
    "        # set up evaluation_interval depending on loss value\n",
    "        cur_interval = -1\n",
    "        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n",
    "            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n",
    "                self.evaluation_interval = interval\n",
    "                cur_interval = i\n",
    "                break\n",
    "#         if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n",
    "#             self.penalty += self.penalize_factor\n",
    "#             self.penalty = min(self.penalty, self.max_penalty)\n",
    "#             self.evaluation_interval += self.penalty\n",
    "#         else:\n",
    "#             self.penalty = 0\n",
    "            \n",
    "        self.prev_loss = last_loss\n",
    "        self.prev_interval = cur_interval\n",
    "        \n",
    "          \n",
    "# 数据导入模型         \n",
    "def make_dataloader(data, tokenizer, is_train=True):\n",
    "    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=Config.max_len)\n",
    "    if is_train:\n",
    "        sampler = RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = SequentialSampler(dataset)\n",
    "\n",
    "    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True)\n",
    "    return batch_dataloader\n",
    "                   \n",
    "# 训练类（包括训练和验证过程）            \n",
    "class Trainer:\n",
    "    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion, model_num):\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = Config.device\n",
    "        self.batches_per_epoch = len(self.train_dl)\n",
    "        self.criterion = criterion\n",
    "        self.model_num = model_num\n",
    "                \n",
    "    def run(self):\n",
    "        record_info = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        evaluation_scheduler = EvaluationScheduler(Config.eval_schedule)\n",
    "        train_loss_counter = AvgCounter()\n",
    "        step = 0\n",
    "        \n",
    "        for epoch in range(Config.epochs):\n",
    "            # 打印每个周期的结果\n",
    "            print(f'{r_}Epoch: {epoch+1}/{Config.epochs}{sr_}')\n",
    "            start_epoch_time = time()\n",
    "            # 训练批次数据\n",
    "            for batch_num, batch in enumerate(self.train_dl):\n",
    "                train_loss = self.train(batch)\n",
    "#                 print(f'{epoch+1}#[{step+1}/{len(self.train_dl)}]: train loss - {train_loss.item()}')\n",
    "\n",
    "                train_loss_counter.update(train_loss, len(batch))\n",
    "                record_info['train_loss'].append((step, train_loss.item()))\n",
    "\n",
    "                if evaluation_scheduler.step(step):\n",
    "                    val_loss = self.evaluate()\n",
    "                    \n",
    "                    record_info['val_loss'].append((step, val_loss.item()))        \n",
    "                    print(f'\\t\\t{epoch+1}#[{batch_num+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n",
    "                    train_loss_counter.reset()\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        print(f\"\\t\\t{g_}Val loss decreased from {best_val_loss} to {val_loss}{sr_}\")\n",
    "                        torch.save(self.model, models_dir / f'best_model_{self.model_num}.pt')\n",
    "                        \n",
    "                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n",
    "                        \n",
    "\n",
    "                step += 1\n",
    "            end_epoch_time = time()\n",
    "            print(f'{bb_}{y_}The epoch took {end_epoch_time - start_epoch_time} sec..{sr_}')\n",
    "\n",
    "        return record_info, best_val_loss\n",
    "            \n",
    "\n",
    "    def train(self, batch):\n",
    "        self.model.train()\n",
    "        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device), \n",
    "        self.model.zero_grad() \n",
    "        preds = self.model(sent_id, mask)\n",
    "        train_loss = self.criterion(preds, labels.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        return torch.sqrt(train_loss)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        val_loss_counter = AvgCounter()\n",
    "\n",
    "        for step,batch in enumerate(self.val_dl):\n",
    "            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                preds = self.model(sent_id, mask)\n",
    "                loss = self.criterion(preds,labels.unsqueeze(1))\n",
    "                val_loss_counter.update(torch.sqrt(loss), len(labels))\n",
    "        return val_loss_counter.avg()\n",
    "    \n",
    "# 计算MSE损失函数    \n",
    "def mse_loss(y_true,y_pred):\n",
    "    return nn.functional.mse_loss(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24f14f18-7620-41dc-be98-522e589a14d7",
    "_uuid": "5e857d09-968b-4593-a035-c066e1690120",
    "execution": {
     "iopub.execute_input": "2021-07-20T09:49:42.606206Z",
     "iopub.status.busy": "2021-07-20T09:49:42.605744Z",
     "iopub.status.idle": "2021-07-20T10:47:06.466119Z",
     "shell.execute_reply": "2021-07-20T10:47:06.463351Z",
     "shell.execute_reply.started": "2021-07-20T09:49:42.606165Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_scores = []\n",
    "# 读取5折模型信息\n",
    "for model_num in range(5): \n",
    "    print(f'{bb_}{w_}  Model#{model_num+1}  {sr_}')\n",
    "    # 使用预训练模型中的分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "    # 使用预训练模型\n",
    "    config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n",
    "    # 参数更新\n",
    "    config.update({\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "            }) \n",
    "    # 读取训练和验证数据\n",
    "    train_dl = make_dataloader(kfold_df[kfold_df.fold!=model_num], tokenizer)\n",
    "    val_dl = make_dataloader(kfold_df[kfold_df.fold==model_num], tokenizer, is_train=False)\n",
    "\n",
    "#     train_dl = make_dataloader(train_df, tokenizer)\n",
    "#     val_dl = make_dataloader(val_df, tokenizer, is_train=False)\n",
    "    # 读取预训练参数\n",
    "    transformer = AutoModel.from_pretrained(Config.pretrained_model_path, config=config)  \n",
    "    # 使用模型\n",
    "    model = CLRPModel(transformer, config)\n",
    "    model = model.to(Config.device)\n",
    "    optimizer = create_optimizer(model)\n",
    "    # 学习率衰减\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_training_steps=Config.epochs * len(train_dl),\n",
    "            num_warmup_steps=50)  \n",
    "\n",
    "    criterion = mse_loss\n",
    "\n",
    "    trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, criterion, model_num)\n",
    "    record_info, best_val_loss = trainer.run()\n",
    "    best_scores.append(best_val_loss)    \n",
    "    \n",
    "    steps, train_losses = list(zip(*record_info['train_loss']))\n",
    "    plt.plot(steps, train_losses, label='train_loss')\n",
    "    steps, val_losses = list(zip(*record_info['val_loss']))\n",
    "    plt.plot(steps, val_losses, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print('Best val losses:', best_scores)\n",
    "print('Avg val loss:', np.array(best_scores).mean())\n",
    "!date '+%A %W %Y %X' > execution_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
