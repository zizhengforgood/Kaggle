{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d5942d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:44.141913Z",
     "iopub.status.busy": "2021-08-18T07:32:44.135981Z",
     "iopub.status.idle": "2021-08-18T07:32:45.337394Z",
     "shell.execute_reply": "2021-08-18T07:32:45.337934Z",
     "shell.execute_reply.started": "2021-08-18T05:29:48.751626Z"
    },
    "papermill": {
     "duration": 1.308091,
     "end_time": "2021-08-18T07:32:45.338241",
     "exception": false,
     "start_time": "2021-08-18T07:32:44.030150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch \n",
    "from PIL import Image\n",
    "import math\n",
    "from random import randint\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "class seti(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_path, trainortest='train', label_path=None, aug=None, cfg=None):\n",
    "        self.img_path = img_path\n",
    "        self.trainortest = trainortest\n",
    "        if trainortest is \"test\":\n",
    "            self.labels = pd.read_csv(label_path)\n",
    "        else: \n",
    "            self.labels = pd.read_csv(img_path)\n",
    "        self.path_labels = [self.img_path + \"train/\" f\"{ids[0]}/{ids}.npy\" for ids in self.labels[\"id\"].values]\n",
    "        self.aims_labels = [ids for ids in self.labels[\"target\"].values.astype(\"f\")]\n",
    "        self.aug = aug\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return transformed image and label for given index.\"\"\"\n",
    "        path = self.paths[index]\n",
    "        img = self._read_cadence_array(path)\n",
    "        if self.aug is not None:\n",
    "            img = self.aug(img.astype(np.uint8))\n",
    "        img = img.float()\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return img, label \n",
    "        return img, f\"{self.paths[index][-19:-4]}\"\n",
    "\n",
    "    def _read_cadence_array(self, path):\n",
    "        \"\"\"Read cadence file and reshape\"\"\"\n",
    "        img = np.load(path)[[0, 2, 4]] \n",
    "        img = np.vstack(img)\n",
    "        img = img.transpose(1, 0)  \n",
    "        img = img.astype(\"f\")[..., np.newaxis]  \n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return num of cadence snippets\"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _resize(self, img_data):\n",
    "        large_edge = max(img_data.shape[0], img_data.shape[1])\n",
    "        resize_factor = self.img_size/large_edge\n",
    "        zero_data = np.zeros((self.img_size, self.img_size,3))\n",
    "        img_data = cv2.resize(img_data, (min(math.ceil(img_data.shape[1]*resize_factor),self.img_size),\n",
    "            min(math.ceil(img_data.shape[0]*resize_factor),self.img_size)))\n",
    "        if img_data.shape[0] >= img_data.shape[1]:\n",
    "            gap = (self.img_size-img_data.shape[1])//2\n",
    "            zero_data[:,gap:img_data.shape[1]+gap,:] = img_data\n",
    "        else:\n",
    "            gap = (self.img_size-img_data.shape[0])//2\n",
    "            zero_data[gap:img_data.shape[0]+gap,:,:] = img_data\n",
    "        return zero_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbf3694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.368504Z",
     "iopub.status.busy": "2021-08-18T07:32:45.367543Z",
     "iopub.status.idle": "2021-08-18T07:32:45.530330Z",
     "shell.execute_reply": "2021-08-18T07:32:45.530821Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.01096Z"
    },
    "papermill": {
     "duration": 0.179116,
     "end_time": "2021-08-18T07:32:45.530984",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.351868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import cv2\n",
    "#from . import seti\n",
    "\n",
    "# LOADER_LUT = {\n",
    "#         'waixingren': waixingren.seti,\n",
    "#     }\n",
    "\n",
    "\n",
    "def get_loader(dataset_type, data_path, loader_type, label_path=None, cfg=None, logger=None):\n",
    "    if loader_type == 'train':\n",
    "        if cfg.USE_AUG == True:\n",
    "            train_aug = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(cfg.ROATION),\n",
    "                #transforms.RandomCrop(cfg.CROP, cfg.PAD),\n",
    "                transforms.RandomResizedCrop(cfg.CROP),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        else:\n",
    "            train_aug = None\n",
    "\n",
    "        try: \n",
    "            _data_class = LOADER_LUT.get(dataset_type)    \n",
    "        except:\n",
    "            logger.error(\"dataset type error, {} not exist\".format(dataset_type))\n",
    "\n",
    "        _data = _data_class(data_path,  dtype='train', label_path=label_path, aug=train_aug, cfg=cfg.RESIZE) \n",
    "        data_loader = torch.utils.data.DataLoader(_data,\n",
    "            batch_size=cfg.BATCHSIZE, shuffle=True, num_workers=cfg.NUM_WORKERS,\n",
    "            drop_last=False)\n",
    "\n",
    "    elif loader_type == 'eval':\n",
    "        if cfg.USE_AUG == True:\n",
    "            val_aug = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(int(cfg.CROP/0.875)),\n",
    "                transforms.CenterCrop(cfg.CROP),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        else:\n",
    "            val_aug = None\n",
    "        try: \n",
    "            _data_class = LOADER_LUT.get(dataset_type)    \n",
    "        except:\n",
    "            logger.error(\"dataset type error, {} not exist\".format(dataset_type))\n",
    "\n",
    "        _data = _data_class(data_path,  dtype='eval', label_path=label_path, aug=val_aug, cfg=cfg.RESIZE) \n",
    "        data_loader = torch.utils.data.DataLoader(_data,\n",
    "            batch_size=cfg.BATCHSIZE, shuffle=False, num_workers=cfg.NUM_WORKERS,\n",
    "            drop_last=False)\n",
    "\n",
    "    elif loader_type == 'self_test':\n",
    "        augmentation = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        try: \n",
    "            _data_class = LOADER_LUT.get(dataset_type)    \n",
    "        except:\n",
    "            logger.error(\"dataset type error, {} not exist\".format(dataset_type))\n",
    "        _data = _data_class(data_path,  dtype='train', label_path=label_path, aug=augmentation, cfg=cfg) \n",
    "        data_loader = torch.utils.data.DataLoader(_data,\n",
    "        batch_size=2, shuffle=True, num_workers=0,\n",
    "        drop_last=True)\n",
    "        \n",
    "    elif loader_type == 'test':\n",
    "        augmentaiton = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        #augmentaiton = None\n",
    "        try: \n",
    "            _data_class = LOADER_LUT.get(dataset_type)    \n",
    "        except:\n",
    "            logger.error(\"dataset type error, {} not exist\".format(dataset_type))\n",
    "        _data = _data_class(data_path, aug=augmentaiton, test_data=True)\n",
    "        data_loader = torch.utils.data.DataLoader(_data,\n",
    "        batch_size=1, shuffle=False, num_workers=0,\n",
    "        drop_last=False)\n",
    "\n",
    "    else:\n",
    "        logger.error(\"error, only support train type dataloader\")\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efd6da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.560943Z",
     "iopub.status.busy": "2021-08-18T07:32:45.559940Z",
     "iopub.status.idle": "2021-08-18T07:32:45.586836Z",
     "shell.execute_reply": "2021-08-18T07:32:45.587331Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.207106Z"
    },
    "papermill": {
     "duration": 0.043302,
     "end_time": "2021-08-18T07:32:45.587538",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.544236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class EffHead(nn.Module):\n",
    "    \"\"\"EfficientNet head: 1x1, BN, Swish, AvgPool, Dropout, FC.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, nc):\n",
    "        super(EffHead, self).__init__()\n",
    "        self.conv = nn.Conv2d(w_in, w_out, 1, stride=1, padding=0, bias=False)\n",
    "        self.conv_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.conv_swish = Swish()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        if 0 > 0.0:\n",
    "            self.dropout = nn.Dropout(0)\n",
    "        self.fc = nn.Linear(w_out, nc, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_swish(self.conv_bn(self.conv(x)))\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x) if hasattr(self, \"dropout\") else x\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, nc):\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 1, 1, 0)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     cx[\"h\"], cx[\"w\"] = 1, 1\n",
    "    #     cx = net.complexity_conv2d(cx, w_out, nc, 1, 1, 0, bias=True)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish activation function: x * sigmoid(x).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation (SE) block w/ Swish: AvgPool, FC, Swish, FC, Sigmoid.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_se):\n",
    "        super(SE, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.f_ex = nn.Sequential(\n",
    "            nn.Conv2d(w_in, w_se, 1, bias=True),\n",
    "            Swish(),\n",
    "            nn.Conv2d(w_se, w_in, 1, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.f_ex(self.avg_pool(x))\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_se):\n",
    "    #     h, w = cx[\"h\"], cx[\"w\"]\n",
    "    #     cx[\"h\"], cx[\"w\"] = 1, 1\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_se, 1, 1, 0, bias=True)\n",
    "    #     cx = net.complexity_conv2d(cx, w_se, w_in, 1, 1, 0, bias=True)\n",
    "    #     cx[\"h\"], cx[\"w\"] = h, w\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    \"\"\"Mobile inverted bottleneck block w/ SE (MBConv).\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, exp_r, kernel, stride, se_r, w_out):\n",
    "        # expansion, 3x3 dwise, BN, Swish, SE, 1x1, BN, skip_connection\n",
    "        super(MBConv, self).__init__()\n",
    "        self.exp = None\n",
    "        w_exp = int(w_in * exp_r)\n",
    "        if w_exp != w_in:\n",
    "            self.exp = nn.Conv2d(w_in, w_exp, 1, stride=1, padding=0, bias=False)\n",
    "            self.exp_bn = nn.BatchNorm2d(w_exp, eps=1e-5, momentum=0.1)\n",
    "            self.exp_swish = Swish()\n",
    "        dwise_args = {\"groups\": w_exp, \"padding\": (kernel - 1) // 2, \"bias\": False}\n",
    "        self.dwise = nn.Conv2d(w_exp, w_exp, kernel, stride=stride, **dwise_args)\n",
    "        self.dwise_bn = nn.BatchNorm2d(w_exp, eps=1e-5, momentum=0.1)\n",
    "        self.dwise_swish = Swish()\n",
    "        self.se = SE(w_exp, int(w_in * se_r))\n",
    "        self.lin_proj = nn.Conv2d(w_exp, w_out, 1, stride=1, padding=0, bias=False)\n",
    "        self.lin_proj_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        # Skip connection if in and out shapes are the same (MN-V2 style)\n",
    "        self.has_skip = stride == 1 and w_in == w_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_x = x\n",
    "        if self.exp:\n",
    "            f_x = self.exp_swish(self.exp_bn(self.exp(f_x)))\n",
    "        f_x = self.dwise_swish(self.dwise_bn(self.dwise(f_x)))\n",
    "        f_x = self.se(f_x)\n",
    "        f_x = self.lin_proj_bn(self.lin_proj(f_x))\n",
    "        if self.has_skip:\n",
    "            f_x = x + f_x\n",
    "        return f_x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, exp_r, kernel, stride, se_r, w_out):\n",
    "    #     w_exp = int(w_in * exp_r)\n",
    "    #     if w_exp != w_in:\n",
    "    #         cx = net.complexity_conv2d(cx, w_in, w_exp, 1, 1, 0)\n",
    "    #         cx = net.complexity_batchnorm2d(cx, w_exp)\n",
    "    #     padding = (kernel - 1) // 2\n",
    "    #     cx = net.complexity_conv2d(cx, w_exp, w_exp, kernel, stride, padding, w_exp)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_exp)\n",
    "    #     cx = SE.complexity(cx, w_exp, int(w_in * se_r))\n",
    "    #     cx = net.complexity_conv2d(cx, w_exp, w_out, 1, 1, 0)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #    return cx\n",
    "\n",
    "\n",
    "class EffStage(nn.Module):\n",
    "    \"\"\"EfficientNet stage.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, exp_r, kernel, stride, se_r, w_out, d):\n",
    "        super(EffStage, self).__init__()\n",
    "        for i in range(d):\n",
    "            b_stride = stride if i == 0 else 1\n",
    "            b_w_in = w_in if i == 0 else w_out\n",
    "            name = \"b{}\".format(i + 1)\n",
    "            self.add_module(name, MBConv(b_w_in, exp_r, kernel, b_stride, se_r, w_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.children():\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, exp_r, kernel, stride, se_r, w_out, d):\n",
    "    #     for i in range(d):\n",
    "    #         b_stride = stride if i == 0 else 1\n",
    "    #         b_w_in = w_in if i == 0 else w_out\n",
    "    #         cx = MBConv.complexity(cx, b_w_in, exp_r, kernel, b_stride, se_r, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class StemIN(nn.Module):\n",
    "    \"\"\"EfficientNet stem for ImageNet: 3x3, BN, Swish.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out):\n",
    "        super(StemIN, self).__init__()\n",
    "        self.conv = nn.Conv2d(w_in, w_out, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.swish = Swish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out):\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 3, 2, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class EffNet(nn.Module):\n",
    "    \"\"\"EfficientNet model.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_args(cfg):\n",
    "        return {\n",
    "            \"stem_w\": cfg.EFFNET.STEM_W,\n",
    "            \"ds\": cfg.EFFNET.DEPTHS,\n",
    "            \"ws\": cfg.EFFNET.WIDTHS,\n",
    "            \"exp_rs\": cfg.EFFNET.EXP_RATIOS,\n",
    "            \"se_r\": cfg.EFFNET.SE_R,\n",
    "            \"ss\": cfg.EFFNET.STRIDES,\n",
    "            \"ks\": cfg.EFFNET.KERNELS,\n",
    "            \"head_w\": cfg.EFFNET.HEAD_W,\n",
    "            \"nc\": cfg.CLASS_NUM,\n",
    "        }\n",
    "\n",
    "    def __init__(self,cfg,logger):\n",
    "        super(EffNet, self).__init__()\n",
    "        self._construct(**EffNet.get_args(cfg))\n",
    "\n",
    "    def _construct(self, stem_w, ds, ws, exp_rs, se_r, ss, ks, head_w, nc):\n",
    "        stage_params = list(zip(ds, ws, exp_rs, ss, ks))\n",
    "        self.stem = StemIN(3, stem_w)\n",
    "        prev_w = stem_w\n",
    "        for i, (d, w, exp_r, stride, kernel) in enumerate(stage_params):\n",
    "            name = \"s{}\".format(i + 1)\n",
    "            self.add_module(name, EffStage(prev_w, exp_r, kernel, stride, se_r, w, d))\n",
    "            prev_w = w\n",
    "        self.head = EffHead(prev_w, head_w, nc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.children():\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx):\n",
    "    #     \"\"\"Computes model complexity. If you alter the model, make sure to update.\"\"\"\n",
    "    #     return EffNet._complexity(cx, **EffNet.get_args())\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _complexity(cx, stem_w, ds, ws, exp_rs, se_r, ss, ks, head_w, nc):\n",
    "    #     stage_params = list(zip(ds, ws, exp_rs, ss, ks))\n",
    "    #     cx = StemIN.complexity(cx, 3, stem_w)\n",
    "    #     prev_w = stem_w\n",
    "    #     for d, w, exp_r, stride, kernel in stage_params:\n",
    "    #         cx = EffStage.complexity(cx, prev_w, exp_r, kernel, stride, se_r, w, d)\n",
    "    #         prev_w = w\n",
    "    #     cx = EffHead.complexity(cx, prev_w, head_w, nc)\n",
    "    #     return cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5418624d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.618378Z",
     "iopub.status.busy": "2021-08-18T07:32:45.616741Z",
     "iopub.status.idle": "2021-08-18T07:32:45.673609Z",
     "shell.execute_reply": "2021-08-18T07:32:45.674086Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.23458Z"
    },
    "papermill": {
     "duration": 0.073571,
     "end_time": "2021-08-18T07:32:45.674275",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.600704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pycls.core.net as net\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def get_stem_fun(stem_type):\n",
    "    \"\"\"Retrieves the stem function by name.\"\"\"\n",
    "    stem_funs = {\n",
    "        \"res_stem_cifar\": ResStemCifar,\n",
    "        \"res_stem_in\": ResStemIN,\n",
    "        \"simple_stem_in\": SimpleStemIN,\n",
    "    }\n",
    "    err_str = \"Stem type '{}' not supported\"\n",
    "    assert stem_type in stem_funs.keys(), err_str.format(stem_type)\n",
    "    return stem_funs[stem_type]\n",
    "\n",
    "\n",
    "def get_block_fun(block_type):\n",
    "    \"\"\"Retrieves the block function by name.\"\"\"\n",
    "    block_funs = {\n",
    "        \"vanilla_block\": VanillaBlock,\n",
    "        \"res_basic_block\": ResBasicBlock,\n",
    "        \"res_bottleneck_block\": ResBottleneckBlock,\n",
    "    }\n",
    "    err_str = \"Block type '{}' not supported\"\n",
    "    assert block_type in block_funs.keys(), err_str.format(block_type)\n",
    "    return block_funs[block_type]\n",
    "\n",
    "\n",
    "class AnyHead(nn.Module):\n",
    "    \"\"\"AnyNet head: AvgPool, 1x1.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, nc):\n",
    "        super(AnyHead, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(w_in, nc, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, nc):\n",
    "    #     cx[\"h\"], cx[\"w\"] = 1, 1\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, nc, 1, 1, 0, bias=True)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class VanillaBlock(nn.Module):\n",
    "    \"\"\"Vanilla block: [3x3 conv, BN, Relu] x2.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, stride, bm=None, gw=None, se_r=None):\n",
    "        err_str = \"Vanilla block does not support bm, gw, and se_r options\"\n",
    "        assert bm is None and gw is None and se_r is None, err_str\n",
    "        super(VanillaBlock, self).__init__()\n",
    "        self.a = nn.Conv2d(w_in, w_out, 3, stride=stride, padding=1, bias=False)\n",
    "        self.a_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.a_relu = nn.ReLU(inplace=True)\n",
    "        self.b = nn.Conv2d(w_out, w_out, 3, stride=1, padding=1, bias=False)\n",
    "        self.b_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.b_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, stride, bm=None, gw=None, se_r=None):\n",
    "    #     err_str = \"Vanilla block does not support bm, gw, and se_r options\"\n",
    "    #     assert bm is None and gw is None and se_r is None, err_str\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 3, stride, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     cx = net.complexity_conv2d(cx, w_out, w_out, 3, 1, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class BasicTransform(nn.Module):\n",
    "    \"\"\"Basic transformation: [3x3 conv, BN, Relu] x2.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, stride):\n",
    "        super(BasicTransform, self).__init__()\n",
    "        self.a = nn.Conv2d(w_in, w_out, 3, stride=stride, padding=1, bias=False)\n",
    "        self.a_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.a_relu = nn.ReLU(inplace=True)\n",
    "        self.b = nn.Conv2d(w_out, w_out, 3, stride=1, padding=1, bias=False)\n",
    "        self.b_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.b_bn.final_bn = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, stride):\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 3, stride, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     cx = net.complexity_conv2d(cx, w_out, w_out, 3, 1, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class ResBasicBlock(nn.Module):\n",
    "    \"\"\"Residual basic block: x + F(x), F = basic transform.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, stride, bm=None, gw=None, se_r=None):\n",
    "        err_str = \"Basic transform does not support bm, gw, and se_r options\"\n",
    "        assert bm is None and gw is None and se_r is None, err_str\n",
    "        super(ResBasicBlock, self).__init__()\n",
    "        self.proj_block = (w_in != w_out) or (stride != 1)\n",
    "        if self.proj_block:\n",
    "            self.proj = nn.Conv2d(w_in, w_out, 1, stride=stride, padding=0, bias=False)\n",
    "            self.bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.f = BasicTransform(w_in, w_out, stride)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.proj_block:\n",
    "            x = self.bn(self.proj(x)) + self.f(x)\n",
    "        else:\n",
    "            x = x + self.f(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, stride, bm=None, gw=None, se_r=None):\n",
    "    #     err_str = \"Basic transform does not support bm, gw, and se_r options\"\n",
    "    #     assert bm is None and gw is None and se_r is None, err_str\n",
    "    #     proj_block = (w_in != w_out) or (stride != 1)\n",
    "    #     if proj_block:\n",
    "    #         h, w = cx[\"h\"], cx[\"w\"]\n",
    "    #         cx = net.complexity_conv2d(cx, w_in, w_out, 1, stride, 0)\n",
    "    #         cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #         cx[\"h\"], cx[\"w\"] = h, w  # parallel branch\n",
    "    #     cx = BasicTransform.complexity(cx, w_in, w_out, stride)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation (SE) block: AvgPool, FC, ReLU, FC, Sigmoid.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_se):\n",
    "        super(SE, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.f_ex = nn.Sequential(\n",
    "            nn.Conv2d(w_in, w_se, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(w_se, w_in, 1, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.f_ex(self.avg_pool(x))\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_se):\n",
    "    #     h, w = cx[\"h\"], cx[\"w\"]\n",
    "    #     cx[\"h\"], cx[\"w\"] = 1, 1\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_se, 1, 1, 0, bias=True)\n",
    "    #     cx = net.complexity_conv2d(cx, w_se, w_in, 1, 1, 0, bias=True)\n",
    "    #     cx[\"h\"], cx[\"w\"] = h, w\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class BottleneckTransform(nn.Module):\n",
    "    \"\"\"Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, stride, bm, gw, se_r):\n",
    "        super(BottleneckTransform, self).__init__()\n",
    "        w_b = int(round(w_out * bm))\n",
    "        g = w_b // gw\n",
    "        self.a = nn.Conv2d(w_in, w_b, 1, stride=1, padding=0, bias=False)\n",
    "        self.a_bn = nn.BatchNorm2d(w_b, eps=1e-5, momentum=0.1)\n",
    "        self.a_relu = nn.ReLU(inplace=True)\n",
    "        self.b = nn.Conv2d(w_b, w_b, 3, stride=stride, padding=1, groups=g, bias=False)\n",
    "        self.b_bn = nn.BatchNorm2d(w_b, eps=1e-5, momentum=0.1)\n",
    "        self.b_relu = nn.ReLU(inplace=True)\n",
    "        if se_r:\n",
    "            w_se = int(round(w_in * se_r))\n",
    "            self.se = SE(w_b, w_se)\n",
    "        self.c = nn.Conv2d(w_b, w_out, 1, stride=1, padding=0, bias=False)\n",
    "        self.c_bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.c_bn.final_bn = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, stride, bm, gw, se_r):\n",
    "    #     w_b = int(round(w_out * bm))\n",
    "    #     g = w_b // gw\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_b, 1, 1, 0)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_b)\n",
    "    #     cx = net.complexity_conv2d(cx, w_b, w_b, 3, stride, 1, g)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_b)\n",
    "    #     if se_r:\n",
    "    #         w_se = int(round(w_in * se_r))\n",
    "    #         cx = SE.complexity(cx, w_b, w_se)\n",
    "    #     cx = net.complexity_conv2d(cx, w_b, w_out, 1, 1, 0)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class ResBottleneckBlock(nn.Module):\n",
    "    \"\"\"Residual bottleneck block: x + F(x), F = bottleneck transform.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, stride, bm=1.0, gw=1, se_r=None):\n",
    "        super(ResBottleneckBlock, self).__init__()\n",
    "        # Use skip connection with projection if shape changes\n",
    "        self.proj_block = (w_in != w_out) or (stride != 1)\n",
    "        if self.proj_block:\n",
    "            self.proj = nn.Conv2d(w_in, w_out, 1, stride=stride, padding=0, bias=False)\n",
    "            self.bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.f = BottleneckTransform(w_in, w_out, stride, bm, gw, se_r)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.proj_block:\n",
    "            x = self.bn(self.proj(x)) + self.f(x)\n",
    "        else:\n",
    "            x = x + self.f(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, stride, bm=1.0, gw=1, se_r=None):\n",
    "    #     proj_block = (w_in != w_out) or (stride != 1)\n",
    "    #     if proj_block:\n",
    "    #         h, w = cx[\"h\"], cx[\"w\"]\n",
    "    #         cx = net.complexity_conv2d(cx, w_in, w_out, 1, stride, 0)\n",
    "    #         cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #         cx[\"h\"], cx[\"w\"] = h, w  # parallel branch\n",
    "    #     cx = BottleneckTransform.complexity(cx, w_in, w_out, stride, bm, gw, se_r)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class ResStemCifar(nn.Module):\n",
    "    \"\"\"ResNet stem for CIFAR: 3x3, BN, ReLU.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out):\n",
    "        super(ResStemCifar, self).__init__()\n",
    "        self.conv = nn.Conv2d(w_in, w_out, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out):\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 3, 1, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class ResStemIN(nn.Module):\n",
    "    \"\"\"ResNet stem for ImageNet: 7x7, BN, ReLU, MaxPool.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out):\n",
    "        super(ResStemIN, self).__init__()\n",
    "        self.conv = nn.Conv2d(w_in, w_out, 7, stride=2, padding=3, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out):\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 7, 2, 3)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     cx = net.complexity_maxpool2d(cx, 3, 2, 1)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class SimpleStemIN(nn.Module):\n",
    "    \"\"\"Simple stem for ImageNet: 3x3, BN, ReLU.\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out):\n",
    "        super(SimpleStemIN, self).__init__()\n",
    "        self.conv = nn.Conv2d(w_in, w_out, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(w_out, eps=1e-5, momentum=0.1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.children():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out):\n",
    "    #     cx = net.complexity_conv2d(cx, w_in, w_out, 3, 2, 1)\n",
    "    #     cx = net.complexity_batchnorm2d(cx, w_out)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class AnyStage(nn.Module):\n",
    "    \"\"\"AnyNet stage (sequence of blocks w/ the same output shape).\"\"\"\n",
    "\n",
    "    def __init__(self, w_in, w_out, stride, d, block_fun, bm, gw, se_r):\n",
    "        super(AnyStage, self).__init__()\n",
    "        for i in range(d):\n",
    "            b_stride = stride if i == 0 else 1\n",
    "            b_w_in = w_in if i == 0 else w_out\n",
    "            name = \"b{}\".format(i + 1)\n",
    "            self.add_module(name, block_fun(b_w_in, w_out, b_stride, bm, gw, se_r))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.children():\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, w_in, w_out, stride, d, block_fun, bm, gw, se_r):\n",
    "    #     for i in range(d):\n",
    "    #         b_stride = stride if i == 0 else 1\n",
    "    #         b_w_in = w_in if i == 0 else w_out\n",
    "    #         cx = block_fun.complexity(cx, b_w_in, w_out, b_stride, bm, gw, se_r)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "class AnyNet(nn.Module):\n",
    "    \"\"\"AnyNet model.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_args(cfg):\n",
    "        return {\n",
    "            \"stem_type\": cfg.ANYNET.STEM_TYPE,\n",
    "            \"stem_w\": cfg.ANYNET.STEM_W,\n",
    "            \"block_type\": cfg.ANYNET.BLOCK_TYPE,\n",
    "            \"ds\": cfg.ANYNET.DEPTHS,\n",
    "            \"ws\": cfg.ANYNET.WIDTHS,\n",
    "            \"ss\": cfg.ANYNET.STRIDES,\n",
    "            \"bms\": cfg.ANYNET.BOT_MULS,\n",
    "            \"gws\": cfg.ANYNET.GROUP_WS,\n",
    "            \"se_r\": cfg.ANYNET.SE_R if cfg.ANYNET.SE_ON else None,\n",
    "            \"nc\": cfg.CLASS_NUM,\n",
    "        }\n",
    "\n",
    "    def __init__(self, cfg, logger, **kwargs):\n",
    "        super(AnyNet, self).__init__()\n",
    "        kwargs = self.get_args(cfg) if not kwargs else kwargs\n",
    "        self._construct(**kwargs)\n",
    "        #self.apply(net.init_weights)\n",
    "\n",
    "    def _construct(self, stem_type, stem_w, block_type, ds, ws, ss, bms, gws, se_r, nc):\n",
    "        # Generate dummy bot muls and gs for models that do not use them\n",
    "        bms = bms if bms else [None for _d in ds]\n",
    "        gws = gws if gws else [None for _d in ds]\n",
    "        stage_params = list(zip(ds, ws, ss, bms, gws))\n",
    "        stem_fun = get_stem_fun(stem_type)\n",
    "        self.stem = stem_fun(3, stem_w)\n",
    "        block_fun = get_block_fun(block_type)\n",
    "        prev_w = stem_w\n",
    "        for i, (d, w, s, bm, gw) in enumerate(stage_params):\n",
    "            name = \"s{}\".format(i + 1)\n",
    "            self.add_module(name, AnyStage(prev_w, w, s, d, block_fun, bm, gw, se_r))\n",
    "            prev_w = w\n",
    "        self.head = AnyHead(w_in=prev_w, nc=nc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.children():\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, **kwargs):\n",
    "    #     \"\"\"Computes model complexity. If you alter the model, make sure to update.\"\"\"\n",
    "    #     kwargs = AnyNet.get_args() if not kwargs else kwargs\n",
    "    #     return AnyNet._complexity(cx, **kwargs)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _complexity(cx, stem_type, stem_w, block_type, ds, ws, ss, bms, gws, se_r, nc):\n",
    "    #     bms = bms if bms else [None for _d in ds]\n",
    "    #     gws = gws if gws else [None for _d in ds]\n",
    "    #     stage_params = list(zip(ds, ws, ss, bms, gws))\n",
    "    #     stem_fun = get_stem_fun(stem_type)\n",
    "    #     cx = stem_fun.complexity(cx, 3, stem_w)\n",
    "    #     block_fun = get_block_fun(block_type)\n",
    "    #     prev_w = stem_w\n",
    "    #     for d, w, s, bm, gw in stage_params:\n",
    "    #         cx = AnyStage.complexity(cx, prev_w, w, s, d, block_fun, bm, gw, se_r)\n",
    "    #         prev_w = w\n",
    "    #     cx = AnyHead.complexity(cx, prev_w, nc)\n",
    "    #     return cx\n",
    "\n",
    "\n",
    "def quantize_float(f, q):\n",
    "    \"\"\"Converts a float to closest non-zero int divisible by q.\"\"\"\n",
    "    return int(round(f / q) * q)\n",
    "\n",
    "\n",
    "def adjust_ws_gs_comp(ws, bms, gs):\n",
    "    \"\"\"Adjusts the compatibility of widths and groups.\"\"\"\n",
    "    ws_bot = [int(w * b) for w, b in zip(ws, bms)]\n",
    "    gs = [min(g, w_bot) for g, w_bot in zip(gs, ws_bot)]\n",
    "    ws_bot = [quantize_float(w_bot, g) for w_bot, g in zip(ws_bot, gs)]\n",
    "    ws = [int(w_bot / b) for w_bot, b in zip(ws_bot, bms)]\n",
    "    return ws, gs\n",
    "\n",
    "\n",
    "def get_stages_from_blocks(ws, rs):\n",
    "    \"\"\"Gets ws/ds of network at each stage from per block values.\"\"\"\n",
    "    ts_temp = zip(ws + [0], [0] + ws, rs + [0], [0] + rs)\n",
    "    ts = [w != wp or r != rp for w, wp, r, rp in ts_temp]\n",
    "    s_ws = [w for w, t in zip(ws, ts[:-1]) if t]\n",
    "    s_ds = np.diff([d for d, t in zip(range(len(ts)), ts) if t]).tolist()\n",
    "    return s_ws, s_ds\n",
    "\n",
    "\n",
    "def generate_regnet(w_a, w_0, w_m, d, q=8):\n",
    "    \"\"\"Generates per block ws from RegNet parameters.\"\"\"\n",
    "    assert w_a >= 0 and w_0 > 0 and w_m > 1 and w_0 % q == 0\n",
    "    ws_cont = np.arange(d) * w_a + w_0\n",
    "    ks = np.round(np.log(ws_cont / w_0) / np.log(w_m))\n",
    "    ws = w_0 * np.power(w_m, ks)\n",
    "    ws = np.round(np.divide(ws, q)) * q\n",
    "    num_stages, max_stage = len(np.unique(ws)), ks.max() + 1\n",
    "    ws, ws_cont = ws.astype(int).tolist(), ws_cont.tolist()\n",
    "    return ws, num_stages, max_stage, ws_cont\n",
    "\n",
    "\n",
    "class RegNet(AnyNet):\n",
    "    \"\"\"RegNet model.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_args(cfg):\n",
    "        \"\"\"Convert RegNet to AnyNet parameter format.\"\"\"\n",
    "        # Generate RegNet ws per block\n",
    "        w_a, w_0, w_m, d = cfg.REGNET.WA, cfg.REGNET.W0, cfg.REGNET.WM, cfg.REGNET.DEPTH\n",
    "        ws, num_stages, _, _ = generate_regnet(w_a, w_0, w_m, d)\n",
    "        # Convert to per stage format\n",
    "        s_ws, s_ds = get_stages_from_blocks(ws, ws)\n",
    "        # Use the same gw, bm and ss for each stage\n",
    "        s_gs = [cfg.REGNET.GROUP_W for _ in range(num_stages)]\n",
    "        s_bs = [cfg.REGNET.BOT_MUL for _ in range(num_stages)]\n",
    "        s_ss = [cfg.REGNET.STRIDE for _ in range(num_stages)]\n",
    "        # Adjust the compatibility of ws and gws\n",
    "        s_ws, s_gs = adjust_ws_gs_comp(s_ws, s_bs, s_gs)\n",
    "        # Get AnyNet arguments defining the RegNet\n",
    "        return {\n",
    "            \"stem_type\": cfg.REGNET.STEM_TYPE,\n",
    "            \"stem_w\": cfg.REGNET.STEM_W,\n",
    "            \"block_type\": cfg.REGNET.BLOCK_TYPE,\n",
    "            \"ds\": s_ds,\n",
    "            \"ws\": s_ws,\n",
    "            \"ss\": s_ss,\n",
    "            \"bms\": s_bs,\n",
    "            \"gws\": s_gs,\n",
    "            \"se_r\": cfg.REGNET.SE_R if cfg.REGNET.SE_ON else None,\n",
    "            \"nc\": cfg.CLASS_NUM,\n",
    "        }\n",
    "\n",
    "    def __init__(self, cfg=None, logger=None):\n",
    "        kwargs = RegNet.get_args(cfg)\n",
    "        super(RegNet, self).__init__(cfg, logger, **kwargs)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def complexity(cx, **kwargs):\n",
    "    #     \"\"\"Computes model complexity. If you alter the model, make sure to update.\"\"\"\n",
    "    #     kwargs = RegNet.get_args() if not kwargs else kwargs\n",
    "    #     return AnyNet.complexity(cx, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2aecfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.703982Z",
     "iopub.status.busy": "2021-08-18T07:32:45.702956Z",
     "iopub.status.idle": "2021-08-18T07:32:45.736181Z",
     "shell.execute_reply": "2021-08-18T07:32:45.736709Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.290792Z"
    },
    "papermill": {
     "duration": 0.04957,
     "end_time": "2021-08-18T07:32:45.736879",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.687309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "__all__ = ['ResNet18','ResNet34','ResNet50','ResNet101','ResNet152']\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,in_dim,out_dim,stride=1,op=\"A\"):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.subconv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim,out_dim,3,stride,1,bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.ReLU(inplace=True),)\n",
    "        self.subconv_2 = nn.Sequential(\n",
    "            nn.Conv2d(out_dim,out_dim,3,1,1,bias=False),\n",
    "            nn.BatchNorm2d(out_dim))\n",
    "        if in_dim == out_dim and stride == 1:\n",
    "            self.downsample = nn.Sequential()\n",
    "        elif op == 'A':\n",
    "            self.downsample =LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, out_dim//4, out_dim//4), \"constant\", 0))\n",
    "        elif op == 'B':\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_dim,out_dim,1,stride,0,bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "        else: raise ValueError\n",
    " \n",
    "    def forward(self,input_):\n",
    "        x_0 = self.subconv_1(input_)\n",
    "        x_1 = self.subconv_2(x_0)\n",
    "        x_input = self.downsample(input_) \n",
    "        x_final = F.relu(x_input + x_1,inplace=True)\n",
    "        return x_final\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self,in_dim,out_dim,stride=1,op=\"B\"):\n",
    "        super(BottleNeck,self).__init__()\n",
    "        self.subconv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim,int(out_dim/self.expansion),1,stride,0,bias=False),\n",
    "            nn.BatchNorm2d(int(out_dim/self.expansion)),\n",
    "            nn.ReLU(inplace=True),)\n",
    "        self.subconv_2 = nn.Sequential(\n",
    "            nn.Conv2d(int(out_dim/self.expansion),\n",
    "                      int(out_dim/self.expansion),3,1,1,bias=False),\n",
    "            nn.BatchNorm2d(int(out_dim/self.expansion)),\n",
    "            nn.ReLU(inplace=True),)\n",
    "        self.subconv_3 = nn.Sequential(\n",
    "            nn.Conv2d(int(out_dim/self.expansion),out_dim,1,1,0,bias=False),\n",
    "            nn.BatchNorm2d(out_dim),)\n",
    "        if in_dim == out_dim and stride == 1:\n",
    "            self.downsample = nn.Sequential()\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_dim,out_dim,1,stride,0,bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self,input_):\n",
    "        x_0 = self.subconv_1(input_)\n",
    "        x_1 = self.subconv_2(x_0)\n",
    "        x_2 = self.subconv_3(x_1)\n",
    "        x_input = self.downsample(input_)\n",
    "        x_final = F.relu(x_input+x_2,inplace=True)\n",
    "        return x_final\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, cfg, logger):\n",
    "        '''\n",
    "        block, BLOCK_LIST, in_dim, \n",
    "        class_num, BASE=64, use_fc=True, CONV1=(7,2,3),\n",
    "        MAX_POOL=True, pretrained=False\n",
    "        '''\n",
    "        super(ResNet,self).__init__()\n",
    "        self.head_conv = nn.Sequential(\n",
    "            nn.Conv2d(cfg.IN_DIM,cfg.BASE,cfg.CONV1[0],cfg.CONV1[1],cfg.CONV1[2],bias=False),\n",
    "            nn.BatchNorm2d(cfg.BASE),\n",
    "            nn.ReLU(inplace=True),)\n",
    "        if cfg.MAX_POOL:\n",
    "            self.maxpool_1 = nn.MaxPool2d(3,2,1)\n",
    "        else:\n",
    "            self.maxpool_1 = nn.Sequential()\n",
    "        block = BottleNeck if cfg.BLOCK == 'bottleneck' else BasicBlock\n",
    "        b_ = block.expansion\n",
    "        self.layer_1 = self._make_layer(block,cfg.BASE,cfg.BASE*b_,cfg.BLOCK_LIST[0],cfg.STRIDE1,cfg.OPERATION)\n",
    "        self.layer_2 = self._make_layer(block,cfg.BASE*b_,cfg.BASE*2*b_,cfg.BLOCK_LIST[1],2,cfg.OPERATION)\n",
    "        self.layer_3 = self._make_layer(block,cfg.BASE*2*b_,cfg.BASE*4*b_,cfg.BLOCK_LIST[2],2,cfg.OPERATION)\n",
    "        self.layer_4 = self._make_layer(block,cfg.BASE*4*b_,cfg.BASE*8*b_,cfg.BLOCK_LIST[3],2,cfg.OPERATION)\n",
    "\n",
    "        final_feature = cfg.BASE*4*b_ if cfg.BLOCK_LIST[3] == 0 else cfg.BASE*8*b_\n",
    "        if cfg.USE_FC:\n",
    "            self.avgpool_1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "            self.fc_1 = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(final_feature,cfg.CLASS_NUM),)\n",
    "        else:\n",
    "            self.avgpool_1 = nn.Sequential()\n",
    "            self.fc_1 = nn.Sequential()\n",
    "        if cfg.DROPOUT > 0: \n",
    "            self.dropout = nn.Dropout(p=cfg.DROPOUT)\n",
    "        else:\n",
    "            self.dropout = nn.Sequential()\n",
    "        self.logger = logger\n",
    "        self.pretrained = cfg.PRETRAINED\n",
    "        self._initialization()\n",
    "    \n",
    "    def _initialization(self):\n",
    "        if self.pretrained is not False:\n",
    "            self.modules.load_state_dict(model_zoo.load_url(model_urls[self.pretrained]))\n",
    "           \n",
    "        else:\n",
    "            for name, sub_module in self.named_modules():\n",
    "                if isinstance(sub_module, nn.Conv2d) or isinstance(sub_module, nn.ConvTranspose2d) or \\\n",
    "                    isinstance(sub_module, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(sub_module.weight)\n",
    "                    # nn.init.kaiming_normal_(sub_module.weight,mode='fan_out'\n",
    "                    #                         ,nonlinearity='relu')\n",
    "                    if self.logger is not None:\n",
    "                        self.logger.info('init {}.weight as kaiming_normal_'.format(name))\n",
    "                    if sub_module.bias is not None:\n",
    "                        nn.init.constant_(sub_module.bias, 0.0)\n",
    "                        if self.logger is not None:\n",
    "                            self.logger.info('init {}.bias as 0'.format(name))\n",
    "                # elif isinstance(sub_module, nn.BatchNorm2d):\n",
    "                #     nn.init.constant_(sub_module.weight,1)\n",
    "                #     nn.init.constant_(sub_module.bias,0)\n",
    "                #     if self.logger is not None:\n",
    "                #         self.logger.info('init {}.weight as constant_ 1'.format(name))\n",
    "                #         self.logger.info('init {}.bias as constant_ 0'.format(name))\n",
    "            \n",
    "    def _make_layer(self,block,in_dim,out_dim,layer_num,stride,op):\n",
    "        net_layers = []\n",
    "        if layer_num == 0:\n",
    "            return nn.Sequential()\n",
    "        else:    \n",
    "            for layer in range(layer_num):\n",
    "                if layer == 0:\n",
    "                    net_layers.append(block(in_dim,out_dim,stride,op))\n",
    "                else:\n",
    "                    net_layers.append(block(out_dim,out_dim,1,op))\n",
    "            return nn.Sequential(*net_layers)\n",
    "                    \n",
    "    def forward(self,input_):\n",
    "        x = self.head_conv(input_)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.avgpool_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_1(x)\n",
    "        return x       \n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "def ResNet18(pretrained = False):\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "def ResNet34(pretrained = False):\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "def ResNet50(pretrained = False):\n",
    "    model = ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "def ResNet101(pretrained = False):\n",
    "    model = ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "def ResNet152(pretrained = False):\n",
    "    model = ResNet(BottleNeck, [3, 8, 36, 3])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36484878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.767020Z",
     "iopub.status.busy": "2021-08-18T07:32:45.766038Z",
     "iopub.status.idle": "2021-08-18T07:32:45.791228Z",
     "shell.execute_reply": "2021-08-18T07:32:45.791752Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.327068Z"
    },
    "papermill": {
     "duration": 0.041975,
     "end_time": "2021-08-18T07:32:45.791924",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.749949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "__all__ = ['ResNeXt50']\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 2\n",
    "    def __init__(self,in_dim,out_dim,stride=1,cardinality=32):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.layers_ = self._make_layers(in_dim,out_dim,stride,cardinality)\n",
    "        if in_dim == out_dim and stride == 1:\n",
    "            self.downsample = None\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_dim,out_dim,1,stride,0,bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "\n",
    "    def _make_layers(self,in_dim,out_dim,stride,cardinality):\n",
    "        layers = []\n",
    "        for group in range(cardinality):\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.BatchNorm2d(in_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_dim,out_dim//self.expansion//cardinality,1,stride,0,bias=False),\n",
    "                nn.BatchNorm2d(out_dim//self.expansion//cardinality),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_dim//self.expansion//cardinality,\n",
    "                out_dim//self.expansion//cardinality,3,1,1,bias=False),\n",
    "                nn.BatchNorm2d(out_dim//self.expansion//cardinality),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_dim//self.expansion//cardinality,out_dim,1,1,0,bias=False),\n",
    "            ))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,input_):\n",
    "        x_input = input_\n",
    "        if self.downsample is not None:\n",
    "            x_input = self.downsample(input_) \n",
    "        x_sum = torch.zeros_like(x_input)\n",
    "        for group in self.layers_:\n",
    "            x_ = group(input_)\n",
    "            x_sum += x_   \n",
    "        x_final = x_input + x_sum\n",
    "        return x_final\n",
    "\n",
    "\n",
    "class BasicBlock_pytorch_group(nn.Module):\n",
    "    expansion = 2\n",
    "    def __init__(self,in_dim,out_dim,stride=1,cardinality=32):\n",
    "        super(BasicBlock_pytorch_group,self).__init__()\n",
    "        self.layers_ = nn.Sequential(\n",
    "                nn.BatchNorm2d(in_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_dim,out_dim//self.expansion,1,stride,0,bias=False),\n",
    "                nn.BatchNorm2d(out_dim//self.expansion),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_dim//self.expansion,\n",
    "                out_dim//self.expansion,3,1,1,bias=False,groups = cardinality),\n",
    "                nn.BatchNorm2d(out_dim//self.expansion),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_dim//self.expansion,out_dim,1,1,0,bias=False),\n",
    "            )\n",
    "        if in_dim == out_dim and stride == 1:\n",
    "            self.downsample = None\n",
    "        else:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_dim,out_dim,1,stride,0,bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self,input_):\n",
    "        x_input = input_\n",
    "        if self.downsample is not None:\n",
    "            x_input = self.downsample(input_) \n",
    "        x_ =  self.layers_(input_)\n",
    "        x_final = x_input + x_\n",
    "        return x_final\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,block_list,cardinality):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.head_conv = nn.Sequential(\n",
    "            nn.Conv2d(3,64,7,2,3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),)\n",
    "        self.maxpool_1 = nn.MaxPool2d(3,2,1)\n",
    "        b_ = block.expansion\n",
    "        self.layer_1 = self._make_layer(block,64,128*b_,block_list[0],1,cardinality)\n",
    "        self.layer_2 = self._make_layer(block,128*b_,256*b_,block_list[1],2,cardinality)\n",
    "        self.layer_3 = self._make_layer(block,256*b_,512*b_,block_list[2],2,cardinality)\n",
    "        self.layer_4 = self._make_layer(block,512*b_,1024*b_,block_list[3],2,cardinality)\n",
    "        self.avgpool_1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*b_,1000),\n",
    "            nn.Softmax(dim = 1),)\n",
    "        self._initialization()\n",
    "    \n",
    "    def _initialization(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance (m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out'\n",
    "                                        ,nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "    \n",
    "    def _make_layer(self,block,in_dim,out_dim,layer_num,stride,cardinality):\n",
    "        net_layers = []\n",
    "        for layer in range(layer_num):\n",
    "            if layer == 0:\n",
    "                net_layers.append(block(in_dim,out_dim,stride,cardinality))\n",
    "            else:\n",
    "                net_layers.append(block(out_dim,out_dim,1,cardinality))\n",
    "        return nn.Sequential(*net_layers)\n",
    "                    \n",
    "    def forward(self,input_):\n",
    "        x = self.head_conv(input_)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.avgpool_1(x)\n",
    "        x = self.fc_1(x)\n",
    "        return x       \n",
    "\n",
    "\n",
    "def ResNeXt50(pretrained = False):\n",
    "    model = ResNet(BasicBlock_pytorch_group, [3, 4, 6, 3],32)\n",
    "    return model\n",
    "\n",
    "    \n",
    "def _test():\n",
    "    from torchsummary import summary\n",
    "    model = ResNeXt50()\n",
    "    model = model.cuda()\n",
    "    summary(model,input_size=(3,224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95407ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.822512Z",
     "iopub.status.busy": "2021-08-18T07:32:45.821886Z",
     "iopub.status.idle": "2021-08-18T07:32:45.829801Z",
     "shell.execute_reply": "2021-08-18T07:32:45.829303Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.353036Z"
    },
    "papermill": {
     "duration": 0.024552,
     "end_time": "2021-08-18T07:32:45.829938",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.805386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from backbone import ResNet2015\n",
    "# from backbone import RegNet2020\n",
    "# from backbone import effnet\n",
    "\n",
    "# NET_LUT = {\n",
    "#         'resnet': ResNet2015.ResNet,\n",
    "#         'regnet': RegNet2020.RegNet,\n",
    "#         'resnext': RegNet2020.AnyNet,\n",
    "#         'effnet': effnet.EffNet,\n",
    "#     }\n",
    "\n",
    "def load_regnet_weight(model,pretrain_path,sub_name):\n",
    "    from collections import OrderedDict\n",
    "    import torch\n",
    "    checkpoints = torch.load(pretrain_path+WEIGHT_LUT[sub_name])\n",
    "    states_no_module = OrderedDict()\n",
    "    for k, v in checkpoints['model_state'].items():\n",
    "        if k != 'head.fc.weight' and k!= 'head.fc.bias':\n",
    "            name_no_module = k\n",
    "            states_no_module[name_no_module] = v\n",
    "    model.load_state_dict(states_no_module,strict=False)\n",
    "\n",
    "\n",
    "# LOAD_LUT = {\n",
    "#         'resnet': ResNet2015.ResNet,\n",
    "#         'regnet': load_regnet_weight,\n",
    "#         'resnext': load_regnet_weight,\n",
    "#         'effnet': load_regnet_weight,\n",
    "#     }\n",
    "\n",
    "# WEIGHT_LUT = {\n",
    "#         'RegNetY-8.0GF': 'regnet/RegNetY-8.0GF_dds_8gpu.pyth',\n",
    "#         'RegNetX-4.0GF': 'regnet/RegNetX-4.0GF_dds_8gpu.pyth',\n",
    "#         'RegNetY-32GF': 'regnet/RegNetY-32GF_dds_8gpu.pyth',\n",
    "#         'ResNeXt-50': 'resnext/X-50-32x4d_dds_8gpu.pyth',\n",
    "#         'EfficientNet-B2': 'effnet/EN-B2_dds_8gpu.pyth',\n",
    "#     }\n",
    "\n",
    "\n",
    "def get_network(net_name, logger=None, cfg=None):\n",
    "    try:\n",
    "        net_class = NET_LUT.get(net_name)\n",
    "    except:\n",
    "        logger.error(\"network tpye error, {} not exist\".format(net_name))\n",
    "    net_instance = net_class(cfg=cfg, logger=logger)\n",
    "    if cfg.PRETRAIN is not None:\n",
    "        load_func = LOAD_LUT.get(net_name)\n",
    "        load_func(net_instance,cfg.PRETRAIN_PATH,cfg.PRETRAIN)\n",
    "        logger.info(\"load {} pretrain weight success\".format(net_name))\n",
    "    return net_instance\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import logging\n",
    "#     from config import cfg, load_cfg\n",
    "#     from ptflops import get_model_complexity_info\n",
    "#     logger = load_cfg() \n",
    "#     model = get_network('resnet', logger=logger, cfg=cfg.MODEL)\n",
    "#     model = model.cuda()\n",
    "#     flops, params = get_model_complexity_info(model,  (3, 224, 224), \n",
    "#         as_strings=True, print_per_layer_stat=True)\n",
    "#     print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "#     print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c63bafa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.862609Z",
     "iopub.status.busy": "2021-08-18T07:32:45.861988Z",
     "iopub.status.idle": "2021-08-18T07:32:45.863776Z",
     "shell.execute_reply": "2021-08-18T07:32:45.864320Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.369905Z"
    },
    "papermill": {
     "duration": 0.021441,
     "end_time": "2021-08-18T07:32:45.864480",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.843039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, t, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    t_a, t_b = t, t[index]\n",
    "    return mixed_x, t_a, t_b, lam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c702da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.894255Z",
     "iopub.status.busy": "2021-08-18T07:32:45.893647Z",
     "iopub.status.idle": "2021-08-18T07:32:45.908237Z",
     "shell.execute_reply": "2021-08-18T07:32:45.908714Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.382406Z"
    },
    "papermill": {
     "duration": 0.030865,
     "end_time": "2021-08-18T07:32:45.908883",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.878018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class WarmLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_lambda, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\n",
    "            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n",
    "        else:\n",
    "            if len(lr_lambda) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n",
    "                    len(optimizer.param_groups), len(lr_lambda)))\n",
    "            self.lr_lambdas = list(lr_lambda)\n",
    "        self.last_epoch = last_epoch\n",
    "        super(WarmLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "\n",
    "    def state_dict(self):\n",
    "        import types\n",
    "        state_dict = {key: value for key, value in self.__dict__.items() if key not in ('optimizer', 'lr_lambdas')}\n",
    "        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)\n",
    "\n",
    "        for idx, fn in enumerate(self.lr_lambdas):\n",
    "            if not isinstance(fn, types.FunctionType):\n",
    "                state_dict['lr_lambdas'][idx] = fn.__dict__.copy()\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        lr_lambdas = state_dict.pop('lr_lambdas')\n",
    "        self.__dict__.update(state_dict)\n",
    "        state_dict['lr_lambdas'] = lr_lambdas\n",
    "\n",
    "        for idx, fn in enumerate(lr_lambdas):\n",
    "            if fn is not None:\n",
    "                self.lr_lambdas[idx].__dict__.update(fn)\n",
    "       \n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > 0:\n",
    "            return [group['lr'] + lmbda(self.last_epoch)\n",
    "                    for lmbda, group in zip(self.lr_lambdas, self.optimizer.param_groups)]\n",
    "        else:\n",
    "            return list(self.base_lrs)\n",
    "\n",
    "\n",
    "def get_opt(model, cfg_train, logger=None, is_warm=False, its_total=0):\n",
    "    trainable_vars = [param for param in model.parameters() if param.requires_grad]\n",
    "    if is_warm:\n",
    "        opt = torch.optim.SGD(trainable_vars, \n",
    "            lr=cfg_train.LR_WARM, \n",
    "            momentum=cfg_train.BETA1,\n",
    "            weight_decay=cfg_train.WEIGHT_DECAY,)\n",
    "        factor = float((cfg_train.LR) / its_total)\n",
    "        lmbda = lambda its: factor\n",
    "        lr_scheduler = WarmLR(opt, lmbda, last_epoch=-1)\n",
    "        return opt, lr_scheduler\n",
    "\n",
    "    if cfg_train.TYPE == 'adamw':\n",
    "        opt = torch.optim.AdamW(trainable_vars, \n",
    "            lr=cfg_train.LR, \n",
    "            betas=(cfg_train.BETA1, cfg_train.BETA2),\n",
    "            eps=1e-08, \n",
    "            weight_decay=cfg_train.WEIGHT_DECAY,\n",
    "            amsgrad=False)\n",
    "    elif cfg_train.TYPE == 'sgd':\n",
    "        opt = torch.optim.SGD(trainable_vars, \n",
    "            lr=cfg_train.LR, \n",
    "            momentum=cfg_train.BETA1,\n",
    "            weight_decay=cfg_train.WEIGHT_DECAY,)\n",
    "    else:\n",
    "        logger.error(\"{} not exist in opt type\".format(cfg_train.TYPE)) \n",
    "\n",
    "    if cfg_train.LR_TYPE == 'cos':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, its_total, eta_min=0, last_epoch=-1)\n",
    "    else:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(opt,cfg_train.LR_REDUCE,\n",
    "                                                        gamma=cfg_train.LR_FACTOR,\n",
    "                                                        last_epoch=-1)\n",
    "    return opt,lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f13cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.937829Z",
     "iopub.status.busy": "2021-08-18T07:32:45.937250Z",
     "iopub.status.idle": "2021-08-18T07:32:45.941831Z",
     "shell.execute_reply": "2021-08-18T07:32:45.942331Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.400504Z"
    },
    "papermill": {
     "duration": 0.020569,
     "end_time": "2021-08-18T07:32:45.942497",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.921928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7498343a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:45.971899Z",
     "iopub.status.busy": "2021-08-18T07:32:45.971308Z",
     "iopub.status.idle": "2021-08-18T07:32:46.930361Z",
     "shell.execute_reply": "2021-08-18T07:32:46.929851Z",
     "shell.execute_reply.started": "2021-08-18T05:29:50.415142Z"
    },
    "papermill": {
     "duration": 0.974816,
     "end_time": "2021-08-18T07:32:46.930499",
     "exception": false,
     "start_time": "2021-08-18T07:32:45.955683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "#-*- coding=utf-8 -*-\n",
    "#python=3.6 pytorch=1.2.0\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "#from easydict import EasyDict as edict\n",
    "from random import randint\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "# from network_factory import get_network\n",
    "# from opt_factory import get_opt\n",
    "# from datasets.loader_factory import get_loader\n",
    "# from test import val\n",
    "# from utils import AUC, AverageMeter,load_cfg,plot_result_data,\\\n",
    "#     load_checkpoints,print_to_screen,save_checkpoints\n",
    "\n",
    "\n",
    "\n",
    "def train(cfg):\n",
    "    logger = load_cfg(cfg)    \n",
    "   \n",
    "    train_loader = get_loader(cfg.DATASET_TRPE, cfg.PATH.DATA, 'train', label_path=cfg.PATH.LABEL, cfg=cfg.TRAIN, logger=logger)\n",
    "    val_loader = get_loader(cfg.DATASET_TRPE, cfg.PATH.EVAL, 'eval',label_path=cfg.PATH.LABEL, cfg=cfg.TRAIN, logger=logger)\n",
    "    its_num = len(train_loader)\n",
    "    # from torchvision import models\n",
    "    # import torch.nn as nn\n",
    "    # class ResNet(nn.Module):\n",
    "    #     def __init__(self, pre_trained=True, n_class=200, model_choice=50):\n",
    "    #         super(ResNet, self).__init__()\n",
    "    #         self.n_class = n_class\n",
    "    #         self.base_model = self._model_choice(pre_trained, model_choice)\n",
    "    #         self.base_model.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    #         self.base_model.fc = nn.Linear(512*4, n_class)\n",
    "    #         #self.base_model.fc.apply(weight_init_kaiming)\n",
    "\n",
    "    #     def forward(self, x):\n",
    "    #         N = x.size(0)\n",
    "    #         assert x.size() == (N, 3, 224, 224)\n",
    "    #         x = self.base_model(x)\n",
    "    #         assert x.size() == (N, self.n_class)\n",
    "    #         return x\n",
    "\n",
    "    #     def _model_choice(self, pre_trained, model_choice):\n",
    "    #         if model_choice == 50:\n",
    "    #             return models.resnet50(pretrained=pre_trained)\n",
    "    #         elif model_choice == 101:\n",
    "    #             return models.resnet101(pretrained=pre_trained)\n",
    "    #         elif model_choice == 152:\n",
    "    #             return models.resnet152(pretrained=pre_trained)\n",
    "\n",
    "    model = get_network(cfg.MODEL.NAME, cfg=cfg.MODEL, logger=logger)\n",
    "    model = torch.nn.DataParallel(model).cuda() \n",
    "   \n",
    "\n",
    "    cfg.TRAIN.LR_REDUCE = [int(its_num*x) for x in cfg.TRAIN.LR_REDUCE]\n",
    "    opt_t,lr_scheduler_t = get_opt(model, cfg.TRAIN, logger, \n",
    "        its_total=(cfg.TRAIN.EPOCHS-cfg.TRAIN.WARMUP)*its_num)\n",
    "    if cfg.TRAIN.WARMUP != 0:\n",
    "        warm_opt, warm_scheduler = get_opt(model, cfg.TRAIN, logger, \n",
    "            is_warm=True, its_total=cfg.TRAIN.WARMUP*its_num)\n",
    "\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    current_epoch = load_checkpoints(model, opt_t, cfg.PATH , logger, lr_scheduler_t)\n",
    "    log_writter = SummaryWriter(cfg.PATH.EXPS+cfg.PATH.NAME)\n",
    "    \n",
    "    auc_total = []\n",
    "    auc_val_total = []\n",
    "    loss_total = []\n",
    "    losss_val_total = []\n",
    "    best_val = [0,0]\n",
    "   \n",
    "    for epoch in range(current_epoch, cfg.TRAIN.EPOCHS):\n",
    "        start_time = time.time()\n",
    "        if epoch < cfg.TRAIN.WARMUP:\n",
    "            opt = warm_opt\n",
    "            lr_scheduler = warm_scheduler\n",
    "        else:\n",
    "            opt = opt_t\n",
    "            lr_scheduler = lr_scheduler_t\n",
    "\n",
    "        auc_train_class = AUC()\n",
    "        loss_train_calss = AverageMeter()\n",
    "        model.train()\n",
    "        data_begin = time.time()\n",
    "        for its, (imgs, targets)in enumerate(train_loader):\n",
    "            data_time = time.time()-data_begin\n",
    "            imgs = imgs.cuda() if torch.cuda.is_available() else imgs\n",
    "            targets = targets.cuda() if torch.cuda.is_available() else targets\n",
    "            \n",
    "                      \n",
    "            opt.zero_grad()\n",
    "            imgs, ta, tb, lam = mixup_data(imgs, targets.view(-1, 1))\n",
    "            outputs = model(imgs)\n",
    "            loss = lam * loss_func(outputs,ta) + (1-lam) * loss_func(outputs,tb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            loss_train_calss.add_value(loss.cpu())\n",
    "            train_time = time.time()-(data_time+data_begin)\n",
    "            data_begin = time.time()\n",
    "            lr = opt.param_groups[0]['lr']\n",
    "            mem = torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0\n",
    "#             try: \n",
    "#                 auc_train_class.add_value(roc_auc_score(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "#             except:\n",
    "#                 auc_train_class.add_value(0.50)                          \n",
    "            if its % cfg.PRINT_FRE == 0:\n",
    "                print_to_screen(loss, lr, its, epoch, its_num, logger, \n",
    "                    data_time,train_time,mem,auc_train_class.print_())\n",
    "            \n",
    "            # if cfg.SHORT_TEST == True:\n",
    "            #     if its == 20:\n",
    "            #         break\n",
    "\n",
    "        save_checkpoints(cfg.PATH.EXPS+cfg.PATH.NAME+cfg.PATH.MODEL, model, opt, epoch,lr_scheduler)\n",
    "        auc_val, loss_val = val(val_loader, model, logger, loss_func, epoch, print_fre=cfg.PRINT_FRE,)\n",
    "        log_writter.add_scalars(\"auc\",{'auc_train':auc_train_class.print_(),\n",
    "                                     'auc_val':auc_val,},\n",
    "                                     epoch)\n",
    "        auc_total.append(auc_train_class.print_())\n",
    "        auc_val_total.append(auc_val)\n",
    "        loss_total.append(loss_train_calss.avg())\n",
    "        losss_val_total.append(loss_val)\n",
    "        end_time = time.time()-start_time\n",
    "        logger.info('Train Auc@1:%.4f\\t'%(auc_train_class.print_())+'Val Auc@1:%.4f\\t'%(auc_val)+'Epoch Time:%.2fmin'%(end_time/60))\n",
    "        if best_val[0] < auc_val:\n",
    "            best_val[0] = auc_val\n",
    "            best_val[1] = epoch\n",
    "            save_checkpoints(cfg.PATH.EXPS+cfg.PATH.NAME+cfg.PATH.BESTMODEL, model, opt, epoch,lr_scheduler)\n",
    "        logger.info('BestV Auc@1:%.4f\\t'%(best_val[0])+\"Best Epoch:%d\"%(best_val[1]))\n",
    "\n",
    "    plot_result_data(auc_total,auc_val_total,loss_total,\n",
    "        losss_val_total,cfg.PATH.EXPS+cfg.PATH.NAME, cfg.TRAIN.EPOCHS)\n",
    "    log_writter.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     from config import cfg\n",
    "#     train(cfg)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b49048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:46.971407Z",
     "iopub.status.busy": "2021-08-18T07:32:46.970770Z",
     "iopub.status.idle": "2021-08-18T07:32:46.974854Z",
     "shell.execute_reply": "2021-08-18T07:32:46.974304Z",
     "shell.execute_reply.started": "2021-08-18T05:34:08.224097Z"
    },
    "papermill": {
     "duration": 0.031366,
     "end_time": "2021-08-18T07:32:46.974974",
     "exception": false,
     "start_time": "2021-08-18T07:32:46.943608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "#from easydict import EasyDict as edict\n",
    "import logging\n",
    "import cv2\n",
    "import time\n",
    "import pandas\n",
    "import csv\n",
    "\n",
    "# from network_factory import get_network\n",
    "# from datasets.loader_factory import get_loader\n",
    "# from utils import load_test_checkpoints, AUC, \\\n",
    "#     AverageMeter, load_cfg, print_to_screen\n",
    "\n",
    "def embedding(test_loader, model_0, model_1, model_2, logger, print_fre=50):\n",
    "    model_0.eval()\n",
    "    model_1.eval()\n",
    "    #model_2.eval()\n",
    "    its_num = len(val_loader)\n",
    "    outputs_all = []\n",
    "    ids_all = []\n",
    "    data_begin = time.time()\n",
    "    with torch.no_grad():\n",
    "        for its, (ids, imgs) in enumerate(test_loader):\n",
    "            data_time = time.time()-data_begin\n",
    "            imgs = imgs.cuda() \n",
    "            outputs_0 = model_0(imgs)\n",
    "            outputs_1 = model_1(imgs)\n",
    "            #outputs_2 = model_2(imgs)\n",
    "            #outputs = (outputs_0.sigmoid() + outputs_1.sigmoid() + outputs_2.sigmoid())/3\n",
    "            outputs = (outputs_0.sigmoid() + outputs_1.sigmoid())/2\n",
    "            loss = 0\n",
    "            train_time = time.time()-(data_time+data_begin)\n",
    "            data_begin = time.time()\n",
    "            mem = torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0\n",
    "            if its % print_fre == 0:\n",
    "                print_to_screen(loss, 0, its, 0, its_num,\n",
    "                    logger, data_time, train_time, mem)  \n",
    "            outputs_all.append(outputs.cpu().numpy())\n",
    "            ids_all.append(ids)\n",
    "    ids_all = np.concatenate(ids_all)        \n",
    "    outputs_all = np.concatenate(outputs_all)\n",
    "    with open (\"./new_embedding.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=' ')\n",
    "        writer.writerow([\"ids\", \"targets\"])\n",
    "        for ids, outputs in zip(ids_all, outputs_all):\n",
    "            writer.writerow([ids, outputs])\n",
    "    \n",
    "\n",
    "\n",
    "def val(val_loader, model, logger=None, loss_function=None, epoch=0, print_fre=50):  \n",
    "    model.eval()\n",
    "    its_num = len(val_loader)\n",
    "    auc_single_val = AverageMeter()\n",
    "    loss_val = AverageMeter()\n",
    "    data_begin = time.time()\n",
    "    with torch.no_grad():\n",
    "        for its, (imgs, targets) in enumerate(val_loader):\n",
    "            data_time = time.time()-data_begin\n",
    "            imgs = imgs.cuda() if torch.cuda.is_available() else imgs\n",
    "            targets = targets.cuda() if torch.cuda.is_available() else targets\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs,targets) if loss_function is not None else torch.tensor(0)\n",
    "            train_time = time.time()-(data_time+data_begin)\n",
    "            data_begin = time.time()\n",
    "            loss_val.add_value(loss.cpu())\n",
    "#             try: \n",
    "#                 auc_single_val.add_value(roc_auc_score(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "#             except:\n",
    "#                 auc_single_val.add_value(0.50)  \n",
    "            mem = torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0\n",
    "            if its % print_fre == 0:\n",
    "                print_to_screen(loss, 0, its, epoch, its_num,\n",
    "                    logger, data_time, train_time, mem, acc=acc_single_val.print_())               \n",
    "    return auc_single_val.print_(), loss_val.avg()        \n",
    "        \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#     if len(sys.argv) < 2:\n",
    "#         print (\"Usage: python test.py [eval, test, embedding]\")\n",
    "#     else:\n",
    "#         cmd = sys.argv[1]\n",
    "#         from config_cub import cfg\n",
    "#         logger = load_cfg(cfg)\n",
    "#         model = get_network(cfg.MODEL.NAME, cfg=cfg.MODEL, logger=logger)\n",
    "#         model = torch.nn.DataParallel(model, cfg.GPUS).cuda() if torch.cuda.is_available() else model\n",
    "#         load_test_checkpoints(model, cfg.PATH, logger, use_best=True)\n",
    "        \n",
    "#         if cmd == 'eval':   \n",
    "#             test_loader = get_loader(cfg.DATASET_TRPE, cfg.PATH.EVAL, 'eval',label_path=cfg.PATH.LABEL, cfg=cfg.TRAIN, logger=logger)\n",
    "#             acc_val, _ = val(test_loader, model, logger, print_fre=cfg.PRINT_FRE,)\n",
    "#             logger.info(\"Prec@1:%.4f\"%(acc_val))\n",
    "\n",
    "#         elif cmd =='test':\n",
    "#             test_loader = get_loader(cfg.DATASET_TRPE, cfg.PATH.TEST, 'test', cfg.TRAIN, logger)\n",
    "#             test(test_loader, model, logger)\n",
    "\n",
    "#         elif cmd =='embedding':\n",
    "#             test_loader = get_loader(cfg.DATASET_TRPE, cfg.PATH.EVAL, 'eval',label_path=cfg.PATH.LABEL, cfg=cfg.TRAIN, logger=logger)\n",
    "#             from config_cub_1 import cfg1\n",
    "#             logger = load_cfg(cfg1)\n",
    "#             model_1 = get_network(cfg1.MODEL.NAME, cfg=cfg1.MODEL, logger=logger)\n",
    "#             model_1 = torch.nn.DataParallel(model_1, cfg1.GPUS).cuda() if torch.cuda.is_available() else model_1\n",
    "#             load_test_checkpoints(model_1, cfg1.PATH, logger, use_best=True)\n",
    "#             test_loader_1 = get_loader(cfg1.DATASET_TRPE, cfg1.PATH.EVAL, 'eval',label_path=cfg1.PATH.LABEL, cfg=cfg1.TRAIN, logger=logger)\n",
    "#             from config_cub_2 import cfg2\n",
    "#             logger = load_cfg(cfg2)\n",
    "#             model_2 = get_network(cfg2.MODEL.NAME, cfg=cfg2.MODEL, logger=logger)\n",
    "#             model_2 = torch.nn.DataParallel(model_2, cfg2.GPUS).cuda() if torch.cuda.is_available() else model_2\n",
    "#             load_test_checkpoints(model_2, cfg2.PATH, logger, use_best=True)\n",
    "#             acc_val = embedding(test_loader, test_loader_1, model,model_1,model_2, logger)\n",
    "#             logger.info(\"Embedding Prec@1:%.4f\"%(acc_val))\n",
    "#         else:\n",
    "#             print (\"Usage: python test.py [eval, test]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc25554a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:47.006214Z",
     "iopub.status.busy": "2021-08-18T07:32:47.005567Z",
     "iopub.status.idle": "2021-08-18T07:32:47.007730Z",
     "shell.execute_reply": "2021-08-18T07:32:47.008142Z"
    },
    "papermill": {
     "duration": 0.020485,
     "end_time": "2021-08-18T07:32:47.008337",
     "exception": false,
     "start_time": "2021-08-18T07:32:46.987852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from easydict import EasyDict as edict\n",
    "\n",
    "\n",
    "# cfg1 = edict()\n",
    "# cfg1.PATH = edict()\n",
    "# cfg1.PATH.DATA =  ['./waixingren/']\n",
    "# cfg1.PATH.LABEL = './waixingren/train_label.csv'\n",
    "# cfg1.PATH.EVAL = ['/waixingren/']\n",
    "# cfg1.PATH.TEST = './waixingren/sample_submission.csv'\n",
    "# cfg1.PATH.RES_TEST = './res_imgs/'\n",
    "# cfg1.PATH.EXPS = './exps/'\n",
    "# cfg1.PATH.NAME = 'seti_effnet'\n",
    "# cfg1.PATH.MODEL = '/model.pth'\n",
    "# cfg1.PATH.BESTMODEL = '/bestmodel.pth'\n",
    "# cfg1.PATH.LOG = '/log.txt'\n",
    "# cfg1.PATH.RESULTS = '/results/'\n",
    "\n",
    "\n",
    "# cfg1.DETERMINISTIC = edict()\n",
    "# cfg1.DETERMINISTIC.SEED = 60\n",
    "# cfg1.DETERMINISTIC.CUDNN = True\n",
    "\n",
    "\n",
    "# cfg1.TRAIN = edict()\n",
    "# cfg1.TRAIN.EPOCHS = 20\n",
    "# cfg1.TRAIN.BATCHSIZE = 32\n",
    "# cfg1.TRAIN.L1SCALING = 100\n",
    "# cfg1.TRAIN.TYPE = 'adamw'\n",
    "# cfg1.TRAIN.LR = 1e-3\n",
    "# cfg1.TRAIN.BETA1 = 0.9\n",
    "# cfg1.TRAIN.BETA2 = 0.999\n",
    "# cfg1.TRAIN.LR_TYPE = 'cos'\n",
    "# cfg1.TRAIN.LR_REDUCE = [26,36]\n",
    "# cfg1.TRAIN.LR_FACTOR = 0.1\n",
    "# cfg1.TRAIN.WEIGHT_DECAY = 1e-4\n",
    "# cfg1.TRAIN.NUM_WORKERS = 16\n",
    "# cfg1.TRAIN.WARMUP = 0\n",
    "# cfg1.TRAIN.LR_WARM = 1e-7\n",
    "# #-------- data aug --------#\n",
    "# cfg1.TRAIN.USE_AUG = True\n",
    "# cfg1.TRAIN.CROP = 620\n",
    "# cfg1.TRAIN.PAD = 0\n",
    "# cfg1.TRAIN.RESIZE = 620\n",
    "# cfg1.TRAIN.ROATION = 45\n",
    "\n",
    "\n",
    "# cfg1.MODEL = edict()\n",
    "# cfg1.MODEL.NAME = 'efficienet'\n",
    "# cfg1.MODEL.IN_DIM = 1\n",
    "# cfg1.MODEL.CLASS_NUM = 1 \n",
    "# cfg1.MODEL.USE_FC = True\n",
    "# cfg1.MODEL.PRETRAIN = True\n",
    "# cfg1.MODEL.PRETRAIN_PATH = './exps/pretrain/'\n",
    "# cfg1.MODEL.DROPOUT = 0.5\n",
    "# cfg1.MODEL.LOSS = 'bce_only_g' \n",
    "# #-------- for resnet --------#\n",
    "# cfg1.MODEL.BLOCK = 'bottleneck'\n",
    "# cfg1.MODEL.BLOCK_LIST = [3,4,6,3] \n",
    "# cfg1.MODEL.CONV1 = (7,2,3)\n",
    "# cfg1.MODEL.OPERATION = 'B'\n",
    "# cfg1.MODEL.STRIDE1 = 1\n",
    "# cfg1.MODEL.MAX_POOL = True\n",
    "# cfg1.MODEL.BASE = 64\n",
    "# #-------- for regnet --------#\n",
    "# cfg1.MODEL.REGNET = edict()\n",
    "# cfg1.MODEL.REGNET.STEM_TYPE = \"simple_stem_in\"\n",
    "# cfg1.MODEL.REGNET.STEM_W = 32\n",
    "# cfg1.MODEL.REGNET.BLOCK_TYPE = \"res_bottleneck_block\"\n",
    "# cfg1.MODEL.REGNET.STRIDE = 2\n",
    "# cfg1.MODEL.REGNET.SE_ON = True\n",
    "# cfg1.MODEL.REGNET.SE_R = 0.25\n",
    "# cfg1.MODEL.REGNET.BOT_MUL = 1.0\n",
    "# cfg1.MODEL.REGNET.DEPTH = 20\n",
    "# cfg1.MODEL.REGNET.W0 = 232\n",
    "# cfg1.MODEL.REGNET.WA = 115.89\n",
    "# cfg1.MODEL.REGNET.WM = 2.53\n",
    "# cfg1.MODEL.REGNET.GROUP_W = 232\n",
    "# #-------- for anynet -------#\n",
    "# cfg1.MODEL.ANYNET = edict()\n",
    "# cfg1.MODEL.ANYNET.STEM_TYPE = \"res_stem_in\"\n",
    "# cfg1.MODEL.ANYNET.STEM_W = 64\n",
    "# cfg1.MODEL.ANYNET.BLOCK_TYPE = \"res_bottleneck_block\"\n",
    "# cfg1.MODEL.ANYNET.STRIDES = [1,2,2,2]\n",
    "# cfg1.MODEL.ANYNET.SE_ON = False\n",
    "# cfg1.MODEL.ANYNET.SE_R = 0.25\n",
    "# cfg1.MODEL.ANYNET.BOT_MULS = [0.5,0.5,0.5,0.5]\n",
    "# cfg1.MODEL.ANYNET.DEPTHS = [3,4,6,3]\n",
    "# cfg1.MODEL.ANYNET.GROUP_WS = [4,8,16,32]\n",
    "# cfg1.MODEL.ANYNET.WIDTHS = [256,512,1024,2048]\n",
    "# #-------- for effnet --------#\n",
    "# cfg1.MODEL.EFFNET = edict()\n",
    "# cfg1.MODEL.EFFNET.STEM_W = 32\n",
    "# cfg1.MODEL.EFFNET.EXP_RATIOS = [1,6,6,6,6,6,6]\n",
    "# cfg1.MODEL.EFFNET.KERNELS = [3,3,5,3,5,5,3]\n",
    "# cfg1.MODEL.EFFNET.HEAD_W = 1408\n",
    "# cfg1.MODEL.EFFNET.DC_RATIO = 0.0\n",
    "# cfg1.MODEL.EFFNET.STRIDES = [1,2,2,2,1,2,1]\n",
    "# cfg1.MODEL.EFFNET.SE_R = 0.25\n",
    "# cfg1.MODEL.EFFNET.DEPTHS = [2, 3, 3, 4, 4, 5, 2]\n",
    "# cfg1.MODEL.EFFNET.GROUP_WS = [4,8,16,32]\n",
    "# cfg1.MODEL.EFFNET.WIDTHS = [16,24,48,88,120,208,352]\n",
    "\n",
    "\n",
    "# cfg1.GPUS = [0]\n",
    "# cfg1.PRINT_FRE = 300\n",
    "\n",
    "# cfg1.DATASET_TRPE = 'waixingren'\n",
    "# cfg1.SHORT_TEST = False\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     from utils import load_cfg1\n",
    "#     logger = load_cfg1(cfg1)\n",
    "#     print(cfg1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "087f64b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:47.040974Z",
     "iopub.status.busy": "2021-08-18T07:32:47.039643Z",
     "iopub.status.idle": "2021-08-18T07:32:47.041597Z",
     "shell.execute_reply": "2021-08-18T07:32:47.042021Z"
    },
    "papermill": {
     "duration": 0.020781,
     "end_time": "2021-08-18T07:32:47.042173",
     "exception": false,
     "start_time": "2021-08-18T07:32:47.021392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from easydict import EasyDict as edict\n",
    "\n",
    "\n",
    "# cfg2 = edict()\n",
    "# cfg2.PATH = edict()\n",
    "# cfg2.PATH.DATA =  ['./waixingren/']\n",
    "# cfg2.PATH.LABEL = './waixingren/train_label.csv'\n",
    "# cfg2.PATH.EVAL = ['/waixingren/']\n",
    "# cfg2.PATH.TEST = './waixingren/sample_submission.csv'\n",
    "# cfg2.PATH.RES_TEST = './res_imgs/'\n",
    "# cfg2.PATH.EXPS = './exps/'\n",
    "# cfg2.PATH.NAME = 'seti_resnext'\n",
    "# cfg2.PATH.MODEL = '/model.pth'\n",
    "# cfg2.PATH.BESTMODEL = '/bestmodel.pth'\n",
    "# cfg2.PATH.LOG = '/log.txt'\n",
    "# cfg2.PATH.RESULTS = '/results/'\n",
    "\n",
    "\n",
    "# cfg2.DETERMINISTIC = edict()\n",
    "# cfg2.DETERMINISTIC.SEED = 60\n",
    "# cfg2.DETERMINISTIC.CUDNN = True\n",
    "\n",
    "\n",
    "# cfg2.TRAIN = edict()\n",
    "# cfg2.TRAIN.EPOCHS = 20\n",
    "# cfg2.TRAIN.BATCHSIZE = 32\n",
    "# cfg2.TRAIN.L1SCALING = 100\n",
    "# cfg2.TRAIN.TYPE = 'adamw'\n",
    "# cfg2.TRAIN.LR = 1e-3\n",
    "# cfg2.TRAIN.BETA1 = 0.9\n",
    "# cfg2.TRAIN.BETA2 = 0.999\n",
    "# cfg2.TRAIN.LR_TYPE = 'cos'\n",
    "# cfg2.TRAIN.LR_REDUCE = [26,36]\n",
    "# cfg2.TRAIN.LR_FACTOR = 0.1\n",
    "# cfg2.TRAIN.WEIGHT_DECAY = 1e-4\n",
    "# cfg2.TRAIN.NUM_WORKERS = 16\n",
    "# cfg2.TRAIN.WARMUP = 0\n",
    "# cfg2.TRAIN.LR_WARM = 1e-7\n",
    "# #-------- data aug --------#\n",
    "# cfg2.TRAIN.USE_AUG = True\n",
    "# cfg2.TRAIN.CROP = 832\n",
    "# cfg2.TRAIN.PAD = 0\n",
    "# cfg2.TRAIN.RESIZE = 832\n",
    "# cfg2.TRAIN.ROATION = 0\n",
    "\n",
    "\n",
    "\n",
    "# cfg2.MODEL = edict()\n",
    "# cfg2.MODEL.NAME = 'resnext'\n",
    "# cfg2.MODEL.IN_DIM = 1\n",
    "# cfg2.MODEL.CLASS_NUM = 1 \n",
    "# cfg2.MODEL.USE_FC = True\n",
    "# cfg2.MODEL.PRETRAIN = True\n",
    "# cfg2.MODEL.PRETRAIN_PATH = './exps/pretrain/'\n",
    "# cfg2.MODEL.DROPOUT = 0\n",
    "# cfg2.MODEL.LOSS = 'bce_only_g' \n",
    "# #-------- for resnet --------#\n",
    "# cfg2.MODEL.BLOCK = 'bottleneck'\n",
    "# cfg2.MODEL.BLOCK_LIST = [3,4,6,3] \n",
    "# cfg2.MODEL.CONV1 = (7,2,3)\n",
    "# cfg2.MODEL.OPERATION = 'B'\n",
    "# cfg2.MODEL.STRIDE1 = 1\n",
    "# cfg2.MODEL.MAX_POOL = True\n",
    "# cfg2.MODEL.BASE = 64\n",
    "# #-------- for regnet --------#\n",
    "# cfg2.MODEL.REGNET = edict()\n",
    "# cfg2.MODEL.REGNET.STEM_TYPE = \"simple_stem_in\"\n",
    "# cfg2.MODEL.REGNET.STEM_W = 32\n",
    "# cfg2.MODEL.REGNET.BLOCK_TYPE = \"res_bottleneck_block\"\n",
    "# cfg2.MODEL.REGNET.STRIDE = 2\n",
    "# cfg2.MODEL.REGNET.SE_ON = True\n",
    "# cfg2.MODEL.REGNET.SE_R = 0.25\n",
    "# cfg2.MODEL.REGNET.BOT_MUL = 1.0\n",
    "# cfg2.MODEL.REGNET.DEPTH = 20\n",
    "# cfg2.MODEL.REGNET.W0 = 232\n",
    "# cfg2.MODEL.REGNET.WA = 115.89\n",
    "# cfg2.MODEL.REGNET.WM = 2.53\n",
    "# cfg2.MODEL.REGNET.GROUP_W = 232\n",
    "# #-------- for anynet -------#\n",
    "# cfg2.MODEL.ANYNET = edict()\n",
    "# cfg2.MODEL.ANYNET.STEM_TYPE = \"res_stem_in\"\n",
    "# cfg2.MODEL.ANYNET.STEM_W = 64\n",
    "# cfg2.MODEL.ANYNET.BLOCK_TYPE = \"res_bottleneck_block\"\n",
    "# cfg2.MODEL.ANYNET.STRIDES = [1,2,2,2]\n",
    "# cfg2.MODEL.ANYNET.SE_ON = False\n",
    "# cfg2.MODEL.ANYNET.SE_R = 0.25\n",
    "# cfg2.MODEL.ANYNET.BOT_MULS = [0.5,0.5,0.5,0.5]\n",
    "# cfg2.MODEL.ANYNET.DEPTHS = [3,4,6,3]\n",
    "# cfg2.MODEL.ANYNET.GROUP_WS = [4,8,16,32]\n",
    "# cfg2.MODEL.ANYNET.WIDTHS = [256,512,1024,2048]\n",
    "# #-------- for effnet --------#\n",
    "# cfg2.MODEL.EFFNET = edict()\n",
    "# cfg2.MODEL.EFFNET.STEM_W = 32\n",
    "# cfg2.MODEL.EFFNET.EXP_RATIOS = [1,6,6,6,6,6,6]\n",
    "# cfg2.MODEL.EFFNET.KERNELS = [3,3,5,3,5,5,3]\n",
    "# cfg2.MODEL.EFFNET.HEAD_W = 1408\n",
    "# cfg2.MODEL.EFFNET.DC_RATIO = 0.0\n",
    "# cfg2.MODEL.EFFNET.STRIDES = [1,2,2,2,1,2,1]\n",
    "# cfg2.MODEL.EFFNET.SE_R = 0.25\n",
    "# cfg2.MODEL.EFFNET.DEPTHS = [2, 3, 3, 4, 4, 5, 2]\n",
    "# cfg2.MODEL.EFFNET.GROUP_WS = [4,8,16,32]\n",
    "# cfg2.MODEL.EFFNET.WIDTHS = [16,24,48,88,120,208,352]\n",
    "\n",
    "\n",
    "# cfg2.GPUS = [0]\n",
    "# cfg2.PRINT_FRE = 300\n",
    "\n",
    "# cfg2.DATASET_TRPE = 'waixingren'\n",
    "# cfg2.SHORT_TEST = False\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     from utils import load_cfg2\n",
    "#     logger = load_cfg2(cfg2)\n",
    "#     print(cfg2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0623db56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T07:32:47.071614Z",
     "iopub.status.busy": "2021-08-18T07:32:47.070961Z",
     "iopub.status.idle": "2021-08-18T07:32:47.242238Z",
     "shell.execute_reply": "2021-08-18T07:32:47.240900Z"
    },
    "papermill": {
     "duration": 0.187052,
     "end_time": "2021-08-18T07:32:47.242410",
     "exception": false,
     "start_time": "2021-08-18T07:32:47.055358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the training and test code are above, we train and test the model on ourselves mechines.\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "results = pd.read_csv(\"/kaggle/input/seti-final/fusion_1.csv\")\n",
    "results.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.725013,
   "end_time": "2021-08-18T07:32:48.908142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-18T07:32:37.183129",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
